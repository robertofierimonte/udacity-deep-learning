{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Classifier in Pytorch\n",
    "---\n",
    "\n",
    "Project #1 of the Udacity Deep Learning Nanodegree\n",
    "\n",
    "Author: **Roberto Fierimonte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from loguru import logger\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from src.models import DenseNet, Lenet5\n",
    "from src.utils import plot_classes_preds, show_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setup\n",
    "data_path = Path.cwd().parent / \"data\"\n",
    "runs_path = Path.cwd().parent / \"runs\"\n",
    "\n",
    "Path.mkdir(data_path, exist_ok=True)\n",
    "Path.mkdir(runs_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading and exploration\n",
    "\n",
    "As a first step we load, display, and analyse the raw training data. Here we do not perform any transformation to the data apart from coverting images to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "logger.info(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = datasets.MNIST(\n",
    "    data_path, download=True, train=True, transform=transforms.ToTensor()\n",
    ")\n",
    "raw_loader = DataLoader(raw_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Number of samples: {len(raw_data)}.\")\n",
    "logger.info(f\"Number of classes: {len(raw_data.classes)}.\")\n",
    "logger.info(f\"Classes: {raw_data.classes}.\")\n",
    "\n",
    "raw_batch = next(iter(raw_loader))[0]\n",
    "logger.info(f\"Shape of batch: {tuple(raw_batch.shape)}.\")\n",
    "logger.info(\n",
    "    f\"Min pixel value: {raw_batch.min().item()}, max pixel value: {raw_batch.max().item()}.\"\n",
    ")\n",
    "\n",
    "raw_grid = utils.make_grid(next(iter(raw_loader))[0])\n",
    "show_grid(raw_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the images are 28 x 28 pixels in size, and they have a single channel. The pixel values are also already normalised between 0 and 1.\n",
    "\n",
    "Based on the fact that we want to classify handwritten digits, we can now think to some data augmentation transformations that we can apply to the training set. The transformations that have identified are:\n",
    "- Random invert: Flips the value of a pixel with probability 0.2.\n",
    "- Random rotation: Rotates the image between -10 and 10 degrees.\n",
    "- Random perspective: Introduces a distortion in the image perspective with probability 0.2.\n",
    "\n",
    "**N.B.:** We should not apply these transformation to the testing and validation sets, as the model needs to be assessed on its performance in the original image space.\n",
    "\n",
    "## Step 2: Model design and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "batch_size = 32  # Batch size\n",
    "n_epochs = 10  # Number of training epochs\n",
    "model = \"densenet\"  # Model type\n",
    "\n",
    "if model not in [\"densenet\", \"lenet5\"]:\n",
    "    raise RuntimeError(\"The model type must be one of ['densenet', 'lenet5'].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%S\")\n",
    "run_path = runs_path / run_name\n",
    "\n",
    "writer = SummaryWriter(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomInvert(p=0.2),\n",
    "        transforms.RandomRotation(degrees=(-10, 10), expand=False),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.2),\n",
    "    ]\n",
    ")\n",
    "train_data = datasets.MNIST(\n",
    "    data_path, download=True, train=True, transform=train_transform\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_data = datasets.MNIST(\n",
    "    data_path, download=True, train=False, transform=transforms.ToTensor()\n",
    ")\n",
    "test_val_generator = torch.Generator().manual_seed(\n",
    "    42\n",
    ")  # We fix the train / test split across multiple runs\n",
    "test_data, val_data = random_split(\n",
    "    test_val_data, lengths=[0.5, 0.5], generator=test_val_generator\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Number of training samples: {len(train_data)}.\")\n",
    "logger.info(f\"Number of validation samples: {len(val_data)}.\")\n",
    "logger.info(f\"Number of test samples: {len(test_data)}.\")\n",
    "\n",
    "train_grid = utils.make_grid(next(iter(train_loader))[0])\n",
    "show_grid(train_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grid = utils.make_grid(next(iter(test_loader))[0])\n",
    "show_grid(test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"densenet\":\n",
    "    net = DenseNet()\n",
    "else:\n",
    "    net = Lenet5()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, input_size=[batch_size, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function (cross-entropy loss) and the optimizer\n",
    "if model == \"densenet\":\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
    "else:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network\n",
    "train_loss_history = list()\n",
    "val_loss_history = list()\n",
    "\n",
    "writer.add_scalar(\"Batch size\", batch_size)\n",
    "writer.add_text(\"Model type\", model)\n",
    "writer.add_image(\"Train images\", train_grid)\n",
    "writer.add_image(\"Test images\", test_grid)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Log the model during the first iteration\n",
    "        if epoch == 0:\n",
    "            writer.add_graph(net, inputs)\n",
    "\n",
    "        # Pass to GPU if available.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_correct += (preds == labels).float().mean().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Log the training stats\n",
    "    writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch + 1)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_correct / len(train_loader), epoch + 1)\n",
    "    logger.info(\n",
    "        f\"Epoch {epoch + 1} training accuracy: {train_correct / len(train_loader):.2%} \"\n",
    "        f\"training loss: {train_loss / len(train_loader):.5f}.\"\n",
    "    )\n",
    "    train_loss_history.append(train_loss / len(train_loader))\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0.0\n",
    "    net.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        val_correct += (preds == labels).float().mean().item()\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    # Log the validation stats\n",
    "    writer.add_scalar(\"Loss/valid\", val_loss / len(val_loader), epoch + 1)\n",
    "    writer.add_scalar(\"Accuracy/valid\", val_correct / len(val_loader), epoch + 1)\n",
    "    logger.info(\n",
    "        f\"Epoch {epoch + 1} validation accuracy: {val_correct / len(val_loader):.2%} \"\n",
    "        f\"validation loss: {val_loss / len(val_loader):.5f}.\"\n",
    "    )\n",
    "    val_loss_history.append(val_loss / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(net.state_dict(), run_path / \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model testing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Neural Network on the test set\n",
    "test_loss = 0.0\n",
    "test_correct = 0.0\n",
    "net.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    test_correct += (preds == labels).float().mean().item()\n",
    "    test_loss += loss.item()\n",
    "\n",
    "# Log the test stats\n",
    "writer.add_scalar(\"Loss/test\", test_loss / len(test_loader), n_epochs)\n",
    "writer.add_scalar(\"Accuracy/test\", test_correct / len(test_loader), n_epochs)\n",
    "logger.info(\n",
    "    f\"Test accuracy: {test_correct / len(test_loader):.2%} \"\n",
    "    f\"test loss: {test_loss / len(test_loader):.5f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actuals_preds = plot_classes_preds(\n",
    "    net=net,\n",
    "    images=test_data.dataset.data[:100].float(),\n",
    "    labels=test_data.dataset.targets[:100],\n",
    "    classes=test_data.dataset.classes,\n",
    ")\n",
    "writer.add_figure(\"Test/predictions vs actuals\", test_actuals_preds, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the tensorboard writer\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-deep-learning-venv",
   "language": "python",
   "name": "udacity-deep-learning-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
